---
title: "Characterizing Contextual Variation in Children's Preschool Language Environment Using Naturalistic Egocentric Videos"
bibliography: bv-preschool-cogsci.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{{\large \bf Robert Z. Sparks, Bria Long, Grace E. Keene, Malia J. Perez,} \\ {\large \bf Alvin W.M. Tan, Virginia A. Marchman, Michael C. Frank} \\ bsparks@stanford.edu, bria@stanford.edu, gkeene@stanford.edu, mjperez@stanford.edu, \\ tanawm@stanford.edu, marchman@stanford.edu, mcfrank@stanford.edu \\ Department of Psychology, Stanford University}

abstract: >
 What structures children’s early language environment? Large corpora of child-centered naturalistic recordings provide an important window into this question, but most available data centers on young children within the home or in lab contexts interacting primarily with a single caregiver. Here, we characterize children’s language experience in a very different kind of environment: the preschool classroom. Children ages 3 – 5 years (N = 26) wore a head-mounted camera in their preschool class, yielding a naturalistic, egocentric view of children’s everyday experience across many classroom activity contexts (e.g., sand play, snack time), with >30 hours of video data. Using semi-automatic transcriptions (227,624 words), we find that activity contexts in the preschool classroom vary in both the quality and quantity of the language that children both hear and produce. Together, these findings reinforce prior theories emphasizing the contribution of activity contexts in structuring the variability in children’s early learning environments.


 
keywords: >
 Classroom Studies, Head-Mounted Cameras, Language Development, Naturalistic Recordings

 
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.crop = TRUE, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
options(scipen=999)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(lme4) # install.packages("lme4", type = "source") - if you get a Matrix error
library(lmerTest)
library(here)
library(egg)
library(ggthemes)
library(koRpus.lang.en)
library(tidyverse)
library(stringr)

```

# Introduction

How do children learn language from their everyday experiences? Children’s experiences are highly varied, characterized by both one-on-one interactions with caregivers, interactions with siblings and peers, overheard interactions between adults or other children, and many other contexts. Yet most theories have been developed via observation of idealized interactions between caregivers and children in experimental contexts [@aslinWhatLook2007; @bergmannPromotingReplicabilityDevelopmental2018]. Acknowledging the richness of children’s home environments, more recent work has made use of home recordings to characterize the developmental experience of infants. Daylong audio recordings [@bergelsonDayDayHour2019; @greenwoodAssessingChildrenHome2011; @vandamHomeBankOnlineRepository2016] and egocentric video recordings of the home using head-mounted cameras [@aslinHowInfantsView2009; @smithContributionsHeadMountedCameras2015; @sullivanSAYCamLargeLongitudinal2021] both provide insight into understanding everyday learning environments and show non-uniformity across developmental experience [@debarbaroTenLessonsInfants2022]. Such data highlight the need to unpack the heterogeneity of the experiences that shape children’s development by building more naturalistic datasets that help to characterize the daylong experiences of children.

Exploring naturalistic accounts of child experience reveal a multitude of different spatial, temporal, and activity contexts, which are associated with correspondingly different language environments. Over the course of a day, children may eat in the dining area, play with toys in the living room, take a bath in the bathroom, and interact with other social partners outside the home. Language input varies across these activity contexts, in terms of the quantity, lexical diversity, mean length of utterance, and semantic content of caregivers’ speech [@bangTimeTalkMultiple2022; @glasActivityTypesChilddirected2018; @tamis-lemondaRoutineLanguageSpeech2019; @hoff1991mother]. This variation in language input also has implications for children’s language acquisition—for example, words used in distinctive contexts (e.g., words strongly associated with the bathroom or kitchen contexts) may be learned earlier than words used across many contexts [@royPredictingBirthSpoken2015]. As such, it is important to understand and characterize children’s learning environments across different activity contexts to capture their true language experiences [@casillasLearningLanguageVivo2023].

However, there is limited work characterizing children’s learning environments across distinct activity contexts outside of the home. Early childhood educational contexts are especially important for early language development. Approximately 54% of preschool-aged children are enrolled in early childhood education (ECE) globally, with some children spending up to 40 hours a week in preschools [@raikesGlobalTrackingAccess2023]. Evidence supporting the effectiveness of preschool on short-term developmental processes (language, cognitive, and social outcomes) and school achievement [@camilliMetaAnalysisEffectsEarly2010; @duncanInvestingPreschoolPrograms2013] motivates a need to appropriately describe the language environments in these ECE contexts. 

Earlier work characterizing children’s preschool learning environments has relied heavily on third-person observation [@sawyerVariationsClassroomLanguage2018]. Video-based observations have largely been limited to full-classroom views focused on head-teacher speech to all children [@dickinsonPatternsTeacherChild2008; @justiceLinguisticEnvironmentPreschool2018]. These macro-level observations are likely a product of technical limitations. Cumbersome and invasive recording technology may prevent researchers from capturing more naturalistic, child-level recordings without being disruptive to the children or the classroom. Obtaining permission from parents both to intervene on normal class time and to record in the classroom provides another potential obstacle to capturing more precise recordings of the preschool language environment. 

A growing body of research has sought to study ECE language environments using child-centered recordings that capture a more holistic view of the preschool classroom inclusive of peer-peer interactions. Technical innovations such as the Language Environment Analysis (LENA) device (http://www.lenafoundation.org/lena-pro/) allow for researchers to capture non-invasive, child-centered audio recordings of children’s environments. Recent work has used this new technology to capture child-centered audio recordings of the preschool language environment [@duncanPredictorsPreschoolLanguage2023; @duncanPrekindergartenClassroomLanguage2020; @perryYearWordsDynamics2018]. Other work has built on existing headcam literature by capturing egocentric video of children’s preschool environments [@chaparro-morenoPreschoolClassroomLinguistic2019]. In addition, recent research has introduced higher-resolution headcams with additional sensors, including gyroscopes and accelerometers, that hold the promise of detailed characterization of children's behavior in richer, more active play contexts [@geanguEgoActiveIntegratedWireless2023; @longBabyViewCameraDesigning2022].  

Just as the home, the preschool classroom is not a uniform context for interactions. Studies using daylong-audio and egocentric video in the preschool classroom demonstrate significant variability in language environments across time, across classrooms, and across individual children within classrooms. Other observational studies demonstrate that early learning in preschool classrooms relies on complex social dynamics involving many different people and activity types [@boorenObservationsChildrenInteractions2012; @sawyerVariationsClassroomLanguage2018]. Related work also suggests that features of classroom language environments can serve as predictors of child language outcomes, such as, vocabulary size [@perryYearWordsDynamics2018; @duncanPredictorsPreschoolLanguage2023]. While prior work has observed variability in the language environment, the specific contributions of activity contexts to ECE language environments are not well characterized. Given that children dynamically shift between many distinct activity contexts across the day (e.g., story time, meals, arts and crafts), it is important to characterize the context-specific language environments that can potentially scaffold early learning in different ways.

In the current work, we use novel video recordings of children’s egocentric perspective to characterize the speech occurring in the dynamic learning environments of a preschool classroom. We record a wide range of experiences in the classroom, including self-guided play, exploration, and interactions both between children, as well as between children and teachers. In total, we recorded from the perspective of 26 unique children on 13 days over a 4 week period, totaling 30.66 hours of observation. Using these egocentric recordings, we were able to label moment-to-moment activity contexts of children as they freely navigated the preschool classroom. We then evaluated both the quality (Lexical Diversity, MLU-w, Lexical Complexity, Keyness) and the quantity (Speech Rate) of the language that children hear and produce. These measures reveal variability in children’s language input and output across activity contexts. Our findings emphasize a non-uniform preschool language environment that changes with the activity contexts in which children engage. 

# Methods

## Participants

34 children from a local nursery school classroom at Stanford University were consented by their parents to be asked to record as a part of this study. This nursery school is a Montessori-like preschool defined by a play-based curriculum and self-guided learning. 26 children (2;11-5;11 years, average age = 4.55 years, 50% male/female, 7.69% African American/Black, 19.2% Asian American/Pacific Islander, 38.5% Caucasian/White, 7.69% Hispanic/Latinx, 30.8% multiracial, 3.85% other) recorded at least one session. 

```{r example-image, fig.env = "figure", fig.pos = "ht!", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "Outdoor (Patio) and indoor (Snack time) child-centered views of the classroom from the BabyView Camera.", fig.width=3, fig.height=3, out.width="75%"}
img <- png::readPNG("figs/example_image.png")
grid::grid.raster(img)
```

<!-- ## Consent process -->

Written consent was provided by one or more parents of each child. A researcher spoke directly with parents outside of normal classroom hours to introduce the study and answer questions. Each parent was shown the camera and given details about how data would be collected and used. Parents provided consent for recording and broad sharing of the resulting data. Before each session, children provided verbal assent to both wear the helmet and make a recording. The child assent process was also detailed to families, and we made it clear that no child would be required to wear the helmet or make a recording. Of the 37 children in the classroom, the parents of three children did not provide consent. Unconsented children remained in the classroom during recordings, and researchers worked with teaching staff to provide opportunities for play away from recordings. Videos were reviewed manually to blackout frames that included unconsented children and to mute portions of audio containing speech from an unconsented child. Parents of unconsented children were made aware of this process before recording began. 

## Procedure

Videos from the child’s perspective were recorded using an adapted version of the BabyView camera [@longBabyViewCameraDesigning2022] (see Figure \ref{fig:example-image} for examples of this view). This head camera is a GoPro Hero 10 Bones camera mounted via a 3D printed fixture on a child safety helmet designed for infants and toddlers. We adjusted the straps and elastic so that helmets would securely and comfortably fit the heads of preschoolers. Children were introduced to the camera as the “Ladybug” across two class sessions. Children were told that if they wore the “Ladybug,” it would make a movie of their play in the classroom. Children were approached and asked if they would like to make a movie, and if they agreed, they confirmed that it was okay to put the helmet on their head. Children were told that they could make a movie for as long as they would like and could stop the recording at any time. Researchers maintained close-distance with the child to allow for monitoring of the child and their camera, but otherwise avoided engagement with the child [^1]. We attempted to record a total of 45 minutes with each child, and on any given recording day, researchers prioritized approaching children who had not yet met this threshold. Recordings were collected during 13 separate sessions over a 4 week period. Up to 3 cameras were in use during any given session. 

[^1]: Research is conducted frequently at this nursery school and researchers are introduced to children as additional “Teachers.” Thus, having researchers in the classroom is normal and children will often approach researchers  as teachers during regular daily activities. 
```{r preload-functions}
source(file=here::here("analysis/analysis_functions.R"))
```

```{r}
# load unfiltered utterance data
unfiltered_data <- read.csv(here("data", "final_transcripts.csv")) %>% filter(!is.na(speaker))
```

```{r load-preprocessed}
# load preprocessed utterancee data
load(file=here::here('data/preprocessed_data.RData'))
```

```{r}
# load unfiltered token data
unfiltered_token_data <- read.csv(here::here("data/token_transcripts.csv")) %>%
 filter(!is.na(token)) 
# %>% mutate(token = tolower(token), lemma = tolower(lemma)) 

# load validated transcirpt
validated_set <- read.csv(here::here("data/validate_transcripts.csv")) %>% group_by(file,vid) %>% 
 mutate(s_length = (max(c(start_time, end_time), na.rm = TRUE) - min(c(start_time, end_time), na.rm=TRUE))) %>% 
 ungroup()

# get token data for sessions greater than 3 minutes
token_data <- unfiltered_token_data %>% 
 filter(s_length >= 180)
```

```{r output-stats}
#Filtered variables for outputting into methods sections
length <- data %>% select(file, s_length) %>% distinct() %>% summarise(sum(s_length)/3600)
total_utterances <- data %>% nrow() %>% as.numeric()

adult_utt <- data %>% filter(speaker %in% c('radult','tadult')) %>% nrow() %>% as.numeric()
radult_utt <- data %>% filter(speaker == "radult") %>% nrow() %>% as.numeric()
tadult_utt <- data %>% filter(speaker == "tadult") %>% nrow() %>% as.numeric()
child_utt <- data %>% filter(speaker %in% c('kchild','ochild')) %>% nrow() %>% as.numeric()
kchild_utt <- data %>% filter(speaker == 'kchild') %>% nrow() %>% as.numeric()
ochild_utt <- data %>% filter(speaker == 'ochild') %>% nrow() %>% as.numeric()

adult_tokens <- token_data %>% filter(speaker %in% c('radult','tadult')) %>% nrow() %>% as.numeric()
kchild_tokens <- token_data %>% filter(speaker == 'kchild') %>% nrow() %>% as.numeric()
ochild_tokens <- token_data %>% filter(speaker == 'ochild') %>% nrow() %>% as.numeric()
total_tokens <- token_data %>% nrow() %>% as.numeric()

# Unfiltered variables 
unfiltered_length <- unfiltered_data %>% select(file, s_length) %>% distinct() %>% summarise(sum(s_length)/60)
total_utterances_unfiltered <- unfiltered_data %>% nrow() %>% as.numeric()
adult_utt_unfiltered <- unfiltered_data %>% filter(speaker %in% c('radult','tadult')) %>% nrow() %>% as.numeric()
child_utt_unfiltered <- unfiltered_data %>% filter(speaker %in% c('kchild','ochild')) %>% nrow() %>% as.numeric()

#Validation Variables 
val_length <- validated_set %>% select(file, vid, s_length) %>% distinct() %>% summarise(sum(s_length)/60)
validated_utt <- validated_set %>% nrow() %>% as.numeric()
val_adult_utt <- validated_set %>% filter(speaker %in% c('radult','tadult')) %>% nrow() %>% as.numeric()
val_child_utt <- validated_set %>% filter(speaker %in% c('kchild','ochild')) %>% nrow() %>% as.numeric()
val_videos <- validated_set %>% select(file, vid) %>% unique() %>% nrow() %>% as.numeric()
val_sessions <- validated_set %>% select(file) %>% unique() %>% nrow() %>% as.numeric()


# Recording Times
session_avg <- data %>% select(file, s_length) %>% distinct() %>% summarise(mean(s_length))/60

avg_total_child <- data %>% select(file, pid, s_length) %>% distinct() %>% group_by(pid) %>% summarise(length = sum(s_length)) %>% summarise(mean(length))/60

range_total_child <- data %>% select(file, pid, s_length) %>% distinct() %>% group_by(pid) %>% summarise(length = sum(s_length))
```

```{r}
# Combine adult codes, add variables
data <- data %>% 
 mutate(speaker = fct_collapse(speaker, "adult" = c("radult","tadult"))) %>%
 filter(!context %in% c('indoor_other','outdoor_other')) %>% # filter "other" contexts
 mutate(indoor_outdoor = str_to_title((str_split_fixed(context,'_',2)[,1]))) %>%
 mutate(location = indoor_outdoor) %>%
 mutate(context_short = str_to_title((str_split_fixed(context,'_',2)[,2]))) %>%
 mutate(context_by_speaker = paste0(context,'-', speaker))
```

```{r}
# add context by speaker and other variables
token_data <- token_data %>% 
 mutate(speaker = fct_collapse(speaker, "adult" = c("radult","tadult"))) 
```

```{r}
# add context by speaker and other variables
token_data <- token_data %>% 
 filter(!context %in% c('indoor_other','outdoor_other')) %>% # filter "other" contexts
  mutate(indoor_outdoor = str_to_title((str_split_fixed(context,'_',2)[,1]))) %>%
  mutate(location = indoor_outdoor) %>%
  mutate(context_short = str_to_title((str_split_fixed(context,'_',2)[,2]))) %>%
  mutate(context_by_speaker = paste0(context,'-', speaker)) 


```

## Data

We initially collected `r length(unique(unfiltered_data$file))` individual sessions resulting in `r round(unfiltered_length/60,2)` hours of recordings. Six sessions were excluded from analyses for being shorter than 3 minutes. Shorter sessions tended to indicate that the child wanted to end the session early and resulted in transcripts mostly involving researcher intervention and child preoccupation with removing the helmet. Thus, the final sample consisted of `r unique(data$file) %>% length()` sessions yielding a total of `r round(length,2)` hours of video data (`r (range(data$s_length)[1]/60) %>% round(2)`-`r (range(data$s_length)[2]/60) %>% round(2)` minutes per session, M = `r round(session_avg, 2)` minutes). Because children could record during multiple recording sessions, on average, individual children contributed `r round(avg_total_child,2)` minutes (range `r round(range(range_total_child$length)[1]/60,2)`-`r round(range(range_total_child$length)[2]/60,2)` minutes). All materials and code for analysis are available at https://osf.io/967zv/ (see Figure \ref{fig:pipeline-image} for overview of analysis pipeline).

```{r pipeline-image, fig.env = "figure", fig.pos = "ht!", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "Overview of the analysis pipeline.", fig.width=3, fig.height=2}

img <- png::readPNG("figs/pipeline.png")
grid::grid.raster(img)
```

<!-- ### Transcription and transcript validation -->

### Transcription 

For each video, we extracted the audio data using FFmpeg, and automatically transcribed the audio using the distil-medium.en model of Distil-Whisper [@gandhiDistilWhisperRobustKnowledge2023], https://huggingface.co/distil-whisper/distil-medium.en, a distilled version of the Whisper model [@radfordRobustSpeechRecognition2022]. Whisper is an automatic speech recognition (ASR) model that allows for transcription of speech audio to text form. While Whisper is generally accurate for adult speech, its training data included relatively few datasets of children’s speech, especially speech from young children; as such, Whisper models are likely to be less robust to child speech [@jainAdaptationWhisperModels2023]. Preschool classrooms are also incredibly noisy, with many children and teachers talking concurrently over ambient noise produced from classroom activities and the surrounding environments (e.g. construction).

### Transcript validation

To ensure reliability of the produced transcripts for analysis, we validated a subset of utterances transcribed by ASR. Each session was divided into video segments of approximately 12 minutes, as a result of the internal caching mechanism of the GoPro camera. For each video segment, we extracted the 30 seconds of video beginning at the midpoint of the video for validation. One of the authors manually annotated the validation set by watching the corresponding section of the video and transcribing the speech. We then computed a Word Error Rate (WER) for the validation set. WER is operationalized as the ratio of the number of word-level errors to the total number of words in the original utterance. WER was calculated using the same WER evaluation model used by Ghandi et. al. in the evaluation of their Distil-Whisper models (https://huggingface.co/spaces/evaluate-metric/wer). 
Across the full dataset, a total of `r total_utterances_unfiltered` utterances (`r adult_utt_unfiltered` adult utterances, `r child_utt_unfiltered` child utterances) were semi-automatically transcribed, prior to data checking and annotation. Of those, a total of `r validated_utt` utterances (`r val_adult_utt` adult utterances, `r val_child_utt` child utterances, `r ((validated_set %>% nrow() / unfiltered_data %>% nrow())*100 ) %>% round(digits=2)`% of total utterances) from `r round(val_length, 2)` minutes (`r ((val_length / unfiltered_length)*100 ) %>% round(digits=2)`% of total recording time) of recordings were validated. We found WER for the full validation set (14.42%), corrected adult utterances (12.31%), and corrected child utterances (18.41%). These results are comparable to long-form evaluation of the distil-medium.en model (12.4%).

### Annotation of speakers and activity contexts

Each video was manually reviewed by a researcher who watched the video alongside the transcript. Video coders assigned a speaker and an activity context for every transcribed utterance. The speaker of each utterance was identified as the key child, another child, a teacher, or a researcher. When the coder identified an utterance that included speech from more than one speaker, utterances were separated into multiple utterances and speakers were assigned accordingly. Each utterance had only one identified speaker. The context was determined by the location of the key child; researchers identified 8 indoor contexts and 5 outdoor contexts [^2], with each context offering distinct affordances and activities. Frequency of utterances and tokens by speaker & context and full descriptions of each code are available on OSF under “annotations”.

[^2]: The "outdoor_hood" context is an abbreviated name for the "Neighborhood", an outdoor pretend play area for children. 

### Data summary

After data-checking and exclusions, `r total_utterances` utterances and `r total_tokens` tokens (words) were included for analysis. A total of `r radult_utt` researcher utterances and `r tadult_utt` teacher utterances were coded. Because children will often interact with researchers as teachers and given the low frequency of researcher utterances, all researcher and teacher utterances were re-coded as Adult utterances. Thus, coded speakers included for analyses were Adults (`r adult_utt` Utterances, `r adult_tokens` Tokens), Key Child (`r kchild_utt` Utterances, `r kchild_tokens` Tokens), and Other Children (`r ochild_utt` Utterances, `r ochild_tokens` Tokens).

## Analysis and Metrics

To evaluate the language environment, we calculated two metrics of speech quantity (utterance rate and token rate), as well as three metrics of speech quality (lexical diversity, grammatical complexity, and mean length of utterance in words). Manually corrected utterances were included in our analyses in place of automatically transcribed utterances. Utterance rate was calculated by dividing the total number of utterances for each speaker by the duration of recordings. Similarly, token rate was calculated by dividing the total number of tokens for each speaker by the duration of recordings. Lexical diversity was operationalized as the measure of textual lexical diversity (MTLD), which reflects the mean number of tokens over which the sample remains above a given type–token ratio threshold; MTLD was chosen as it is less sensitive to differences in transcript length [@mccarthyAssessmentRangeUsefulness2005; @mccarthyMTLDVocdDHDD2010], which was expected in our sample due to the large range in session duration. Grammatical complexity was operationalized as the proportion of complex utterances, calculated by dividing the number of utterances with more than one verb by the total number of utterances. Finally, mean length of utterance in words (MLU-w) was calculated by dividing the total number of word tokens by the total number of utterances. 

To characterize the language used in different contexts, we identified words which were most representative of each context, following the method used in @dawsonFeaturesLexicalRichness2021 and @kilgarriffSimpleMathsKeywords2009. We calculated a keyness score for each word for each context, using speech occurring within that context as the focus corpus and speech occurring within all other contexts as the reference corpus. The keyness score was calculated as the ratio of the normalized frequency of the word in the focus corpus to the normalized frequency in the reference corpus, using average reduced frequencies (instead of raw frequencies) to account for the dispersion of the word over the episode [@hlavacovaNewApproachFrequency2006]. We extracted the 10 words with the highest keyness for each context as a means of visualizing how different contexts may elicit the use of different vocabulary items.

# Results

```{r}
session_data <- data %>% select(file, age, pid, age_group, gender, s_length) %>% distinct()
```

```{r}

data_by_context_by_speaker_summary <- token_data %>%
  group_by(context_by_speaker, context, context_short, indoor_outdoor, speaker) %>%
  summarize(s_length = mean(s_length)) 

token_data_by_context_by_speaker_summary <- token_data %>%
  group_by(context_by_speaker, context, context_short, indoor_outdoor, speaker) %>%
  summarize(num_tokens = length(token)) 

file_token_data_by_context_by_speaker_summary <- token_data %>%
  group_by(context_by_speaker, file, context, context_short, indoor_outdoor, speaker) %>%
  summarize(num_tokens = length(token))

```

```{R}
## normed tokens by age
sp_tn <- token_data %>% 
  group_by(speaker) %>% 
  summarise(get_norm_token(cur_data())) %>%
  mutate(token_norm = token_norm*60) %>%
  left_join(session_data)
```

```{r tokens-by-age, fig.env="figure", fig.pos = "b!", fig.align = "center", fig.width = 3.5, fig.height = 3, out.width="70%", fig.cap = "Token rate produced by each child during each session as a function of the age of the child wearing the headcam. Lines connect sessions from individual participants. Dots are scaled by the length of the sessions."}
ggplot(data=sp_tn %>% 
         filter(speaker=='kchild'), aes(x=age, y=token_norm, col=speaker)) +
  geom_point(alpha=.4, aes(size=s_length)) +
  scale_color_manual(values = c("#7570B3")) +
  geom_line(aes(group=pid), alpha=.2) +
  theme(legend.position = 'none') + 
  theme_few() +
  ylab('# Tokens / Minute') + 
  ylim(0,60) +
  xlim(2.8, 6) +
  # scale_y_continuous(breaks=c(0,15,30,45,60))+ theme_few(base_size=10) +
  geom_smooth(aes(group=speaker, weight=s_length), span=20, method='lm') +
  xlab('Age (in years)') +
  theme(legend.position = 'none')
```

```{r}
context_by_speaker_lengths <- data %>% 
  mutate(context_by_speaker = paste0(context,'-', speaker), utt_length = end_time - start_time) %>%
  group_by(context_by_speaker) %>% 
  mutate(s_length = sum(utt_length, na.rm = TRUE)) %>% 
  select(context_by_speaker, s_length) %>% 
  distinct()
```

```{r}
tn_by_context_by_speaker <- token_data %>% 
  mutate(file = context_by_speaker) %>% select(-(s_length)) %>% 
  left_join(context_by_speaker_lengths, by="context_by_speaker") %>% 
  get_norm_token() %>% 
  mutate(token_norm = token_norm*60) %>%
  rename(context_by_speaker = file) %>% 
  ungroup() %>%
  left_join(token_data_by_context_by_speaker_summary) %>%
  group_by(context_short) %>%
  mutate(avg_tokens_across_speakers = mean(token_norm)) %>%
  ungroup() %>%
  mutate(context_short = fct_reorder(context_short, avg_tokens_across_speakers, .desc=TRUE)) 


```

```{r tokens-by-context, fig.env="figure", fig.pos = "b!", fig.align = "center", fig.cap = "Token rate produced by each child wearing the headcam, other children in their environment, and adults as a function of activity contexts. Dots are scaled by the number of tokens produced overall.", fig.width = 3.5, fig.height = 3, out.width="70%" }

levels(tn_by_context_by_speaker$speaker) = (labels = c('Key child','Other children','Adult'))

ggplot(data=tn_by_context_by_speaker, aes(x=context_short, y=token_norm, color=speaker)) +
  geom_point(aes(size=num_tokens), alpha=.6) +
  theme_few(base_size=10) +
  coord_flip() +
  xlab('') +
  ylab('# Tokens / Minute') +
  facet_grid(scales="free_y", space = "free", rows=vars(indoor_outdoor)) +
  scale_color_brewer(name="", palette='Dark2', direction=-1) +
  scale_size_area(guide='none') +
  theme(legend.position = 'bottom')
```

```{r}
ld_by_context_by_speaker_temp <- token_data %>%
  mutate(file = context_by_speaker) %>%
  get_ld(measure = "MTLD") %>%
  rename(file = file_name, ld = value, context_by_speaker = file_name)
```

```{r}
ld_by_context_by_speaker <- ld_by_context_by_speaker_temp %>%
  ungroup() %>%
  left_join(token_data_by_context_by_speaker_summary) %>%
  group_by(context) %>%
  mutate(avg_ld = mean(ld)) %>%
  ungroup() %>%
  left_join(token_data_by_context_by_speaker_summary) %>%
  ungroup %>%
  mutate(context_short = fct_reorder(context_short, avg_ld, .desc=TRUE))

```

```{r lexical-diversity-metrics}
# lexical diversity by context/speaker
location_avg <- ld_by_context_by_speaker %>%
  group_by(indoor_outdoor, speaker) %>%
  summarize(mean_ld = weighted.mean(ld, num_tokens))
# 
data_figure_2a <- ggplot(data=ld_by_context_by_speaker, aes(x=context_short, y=ld, color=speaker)) +
  geom_point(aes(size=num_tokens), alpha=.6) +
  theme_few(base_size=10) +
  theme(axis.text.x = element_text(angle = 45, vjust = .9, hjust=1)) +
  xlab('') +
  ylab('Lexical Diversity (MTLD)') +
  facet_grid(~indoor_outdoor, scales="free_x", space = "free") +
  scale_color_brewer(name="", palette='Dark2', direction=-1) +
  theme(legend.position = 'none') +
  geom_hline(data = location_avg, lty = 2, alpha = .5, aes(yintercept = mean_ld, col=speaker))

```

```{r mlu-metrics}



# MLU by context/speaker
mlu_by_context_by_speaker <- data %>% 
  get_mlu(by = 'context_by_speaker') %>% 
  ungroup() %>%
  left_join(token_data_by_context_by_speaker_summary) %>%
  group_by(context) %>%
  mutate(avg_mlu = mean(mlu)) %>%
  ungroup() %>%
  mutate(context_short = fct_reorder(context_short, avg_mlu, .desc=TRUE)) 

location_avg <- mlu_by_context_by_speaker %>%
  group_by(indoor_outdoor, speaker) %>%
  summarize(mean_mlu = weighted.mean(mlu, num_tokens))

data_figure_2b = ggplot(data=mlu_by_context_by_speaker, aes(x=context_short, y=mlu, color=speaker)) +
  geom_point(aes(size=num_tokens), alpha=.6) +
  theme_few(base_size=10) +
  theme(axis.text.x = element_text(angle = 45, vjust = .9, hjust=1)) +
  xlab('') +
  ylab('Mean Length of Utterance') +
  facet_grid(~indoor_outdoor, scales="free_x", space="free") +
  scale_color_brewer(name="", palette='Dark2', direction=-1) +
  theme(legend.position = 'none') +
  geom_hline(data = location_avg, lty = 2, alpha = .5, aes(yintercept = mean_mlu, col=speaker))

```

```{r complexity-metrics}
# complexity
complexity_by_context_by_speaker <- token_data %>% 
  mutate(file = context_by_speaker) %>% 
  get_complexity() %>% 
  rename(complexity = value, context_by_speaker = file) %>% 
  ungroup() %>%
  left_join(token_data_by_context_by_speaker_summary) %>%
  group_by(context) %>%
  mutate(avg_complexity = mean(complexity)) %>%
  ungroup() %>%
  mutate(context_short = fct_reorder(context_short, avg_complexity, .desc=TRUE))

location_avg <- complexity_by_context_by_speaker %>%
  group_by(indoor_outdoor, speaker) %>%
  summarize(mean_complexity = weighted.mean(complexity, num_tokens))

data_figure_2c = ggplot(data=complexity_by_context_by_speaker, aes(x=context_short, y=complexity, color=speaker)) +
  geom_point(aes(size=num_tokens), alpha=.6) +
  theme_few(base_size=10) +
  theme(axis.text.x = element_text(angle = 45, vjust = .9, hjust=1)) +
  xlab('') +
  ylim(0,1) +
  ylab('Complexity') +
  facet_grid(~indoor_outdoor, scales="free_x", space='free') +
  scale_color_brewer(name="", palette='Dark2', direction=-1) +
  theme(legend.position = 'none') +
  geom_hline(data = location_avg, lty = 2, alpha = .5, aes(yintercept = mean_complexity, col=speaker))

```

```{r quality-by-context, fig.env="figure*", fig.pos = "h!", fig.align = "center", fig.width=6.5, fig.height=2.5, fig.cap = "Variation in the quality of speech across different activity contexts. Colors indicate speaker types (green=adult, purple=key child, orange=other children). Dots are scaled by the number of tokens included in each speaker/context combination; dashed lines indicate weighted-average values across activity contexts within indoor/outdoor locations.", out.width="80%"} 

ggarrange(data_figure_2a,data_figure_2b, data_figure_2c, nrow=1)
```

```{r ld-metrics-by-age}

# Now look also at main metrics by age. 

#LD by speaker by age
sp_ld <- data.frame()
for (x in unique(token_data$speaker)) {
  ld_frame <- token_data %>%
    ungroup() %>%
    filter(speaker == x) %>%
    get_ld(measure = "MTLD") %>%
    rename(file = file_name, ld = value) %>%
    mutate(speaker = x) %>%
    select(speaker, file, ld)
  sp_ld <- rbind(sp_ld, ld_frame)
}
sp_ld[sapply(sp_ld, is.infinite)] <- NA
```

```{r mlu-by-age}
# MLU by speaker age
mlu_by_speaker_by_file <- data %>% 
  get_mlu(by = c("speaker","file")) %>%
  left_join(session_data) %>%
  filter(speaker=='kchild')
```

```{r complexity-by-age}
#complexity by speaker by age
sp_complexity <- data.frame()
for (x in unique(token_data$speaker)) {
  c_frame <- token_data %>%
    ungroup() %>%
    filter(speaker == x) %>%
    get_complexity() %>%
    rename(prop_complex = value) %>%
    mutate(speaker = x) %>%
    select(speaker, file, prop_complex)
  sp_complexity <- rbind(sp_complexity, c_frame)
}
sp_complexity[sapply(sp_complexity, is.infinite)] <- NA
```

```{r speaker-summary}

## DONE ONLY FOR KCHILD
by_file_summary <- mlu_by_speaker_by_file %>%
  left_join(sp_ld,by=c('file','speaker')) %>%
  left_join(sp_complexity, by=c('file','speaker'))
```

```{r inferential-stats-for-age, echo=FALSE}
mlu_by_age_out = summary(lmer(data=by_file_summary, mlu ~ scale(age) + s_length + (1|pid)))
ld_by_age_out = summary(lmer(data=by_file_summary, ld ~ scale(age) + s_length + (1|pid)))
complexity_by_age_out = summary(lmer(data=by_file_summary, prop_complex ~ scale(age) + s_length + (1|pid)))
```

```{r load-keyness}
# load keyness data
load(file=here::here('data/context_keyness.RData'))

# make nice column names
keyness_prettified <- keyness_file 
colnames(keyness_prettified) <- paste0(str_to_title(str_split_fixed(colnames(keyness_file),'_',2)[,2]))


```

```{r keyness_table, results="asis"}
print(xtable(keyness_prettified, caption ="Word keyness by activity context", label="tab:keyness_table"), floating.environment="table*", type="latex", comment=F, scalebox='0.65', table.placement="ht!")

```

Using these coded transcripts, we characterized the consistency and variability in the preschool language environment across activity contexts, speakers, and the age of the child wearing the head-mounted camera.

First, we examined variation in the quantity of the language children produced and heard, across both the age of the target child as well as the activity contexts in which they were embedded. Older children tended to produce more tokens over time than younger children, consistent with the general observation that older preschoolers tend to be more proficient in language production (see Figure \ref{fig:tokens-by-age}). Still, we observed great variability within-child, suggesting that age alone is not a good predictor of speech quantity. Moreover, there was wide variation across activity contexts. Some activities tended to elicit more speech overall than others, both from children, their peers, and adults. Block-building, for example, tended to have overall less speech produced and heard than either arts & crafts or snacktime (see Figure \ref{fig:tokens-by-context}). A similar pattern held for the number of utterances produced over time.

Next, we examined how metrics of language quality varied across both contexts and target child age, focusing on measures of lexical diversity, grammatical complexity, and average utterance length (MLU-w). Given the limited amount of data by context, only descriptive statistics were calculated for each metric. We observed variability in each metric across the different activity contexts: for example, while lexical diversity was relatively high during literacy activities in the indoor news context from both adults and children, outdoor grass-play had relatively low lexical diversity (see Figure \ref{fig:quality-by-context}). We found similar trends when we examined both MLU as well as grammatical complexity: different activities across indoor and outdoor contexts -- typically those heavily scaffolded by adult caregivers -- tended to elicit longer, more grammatically complex utterances from teachers and children.

In contrast, we found only weak evidence for changes in these metrics by the age of the target child: while the average MLU-w and grammatical complexity increased numerically with age, we did not observe discernible variation in lexical diversity, and none of these metrics reached statistical significance in linear mixed-effect models (all $P$ > .06).

Finally, we analyzed the content of the conversations in each activity context by identifying words that were most representative of each context in this corpus (see Table \ref{tab:keyness_table}). For example: the number one keyword in the puzzle context was "puzzle", keywords during snack include many food-items, and keywords in outdoor sand play allude to the activities of the context such as digging for gems or digging a river.  

# Discussion

We examined children's language environments in a preschool classroom. By combining a new, high-resolution camera with an extensive consent process and AI-driven transcription workflow, we were able to curate a dataset that captured a less-studied, but frequently-experienced part of children's daily linguistic experiences. We were also able to quantify variance in both caregiver and child speech across different activity contexts within the classroom (e.g., indoor block building vs. outdoor sand play). While prior work emphasized the importance of activity contexts in shaping linguistic experiences in home environments, our work extends our understanding of how a variety of activity contexts contribute to linguistic experiences beyond the home, namely in the preschool classroom.

While the age of the child explained some variability in the language environment, activity contexts were a source of much greater variability in both the quantity and quality of language in the preschool classroom. Our keyness results provide evidence for specific activities shaping the content of speech in the classroom. Although these results are likely to be classroom-specific, they provide confirmation of emerging context-specific lexicons across the contexts in which children and teachers were engaged. We also observed variability in speech rate, lexical diversity, MLU-w, and grammatical complexity across activity contexts, providing further evidence that different activities elicit speech from both children and teachers that differ along many dimensions.

Our lack of a developmental effect is perhaps surprising in light of the obvious growth in children's language in the preschool period (ages 3--5). In our dataset, older children were more likely to produce more speech (i.e., more speech tokens) than younger children, but we did not find large differences as a function of age in either grammatical complexity, mean utterance length, or lexical diversity. Some of this pattern may be explained by individual variation: a wealth of evidence demonstrates that children within a given age band vary greatly in their productive language ability [@frank2017wordbank]. We also noticed that individual sessions from the same participant could vary greatly in the quantity and quality of speech that was recorded -- again suggesting that different activities or social environments may better explain variance and be larger drivers of variability in the language environment. 

These findings build on prior work that aims to better characterize the preschool language environment [@justiceLinguisticEnvironmentPreschool2018; @chaparro-morenoPreschoolClassroomLinguistic2019]. While recent work has emphasized the collection of child-centered audio recordings, we extend a relatively new area of research that collects egocentric video data of children's preschool environments. These video data allow for moment-to-moment access to child activity in the classroom across many days, permitting access to observable variability in the language environment across time and contexts.

Although we are working with naturalistic data, there are various limitations on the generalizability of these findings that future work may address. We recruited children from a single classroom in a well-resourced nursery school where research is frequently conducted. Additionally, child activity at this school is mostly self-guided, which is a departure from other preschool curricula that emphasize teacher-led activities. Future work should explore other types of preschool populations and ECE programs to better characterize preschool language environments more generally, and how those environments might vary across curriculum, available resources, and sociocultural context. 

Additionally, our data were collected from a relatively small sample of 25 children over 4 weeks. More video data from more children across a longer timescale may better elucidate both individual variability and within-child variability within their language environment. More data across activity contexts would also allow for the calculation of inferential statistics that may provide more concrete evidence of contextual variability. In this work, we also focused on specific metrics of language quantity and quality that were directly observed in the language environment. Other metrics of language ability (e.g. receptive vocabulary) or social cognition may help to explain variance in the child's language environment. Future work will continue to build on this corpus by recording more video in a preschool classroom across the full school year. Additionally, while present analyses focused exclusively on speech, future work might emphasize multimodal experience (e.g. visual and kinesthetic experience) that can be measured from video recordings as pathways to characterizing the child's learning environment more holistically.

More broadly, this work aims to characterize the structure of variability in children's early language learning environments. We build on prior theories of context-specific language learning [@bangTimeTalkMultiple2022] and work emphasizing the importance of these contexts for early vocabulary growth [@royPredictingBirthSpoken2015], showing that activity context structures children's immediate language environment beyond the home-environment in infancy into the preschool classroom. Prior work emphasizes a need to consider language learning "in-vivo" [@casillasLearningLanguageVivo2023]. To fully capture sources of variability in children's learning environments, theories must take into account not only what caregivers and children say, but also what they do. Our findings provide empirical evidence of this view, reinforcing language learning as an ecologically-driven process where interactions between children and their environment create context-specific opportunities for language learning. 

While the present work provides evidence of contextual variability within the preschool language environment, future work might aim to use these methods to more specifically characterize the structure of specific activity contexts in the classroom. The classroom language environment is an important driver of language outcomes for children [@duncanPredictorsPreschoolLanguage2023; @perryYearWordsDynamics2018]. Understanding how specific contexts shape language may allow for better scaffolding of activities that target language development in the classroom. 

This work also seeks to broaden the kind of naturalistic corpora that we develop for characterizing the developmental experience of young children [@debarbaroTenLessonsInfants2022]. Building corpora that center on distinct contexts in which children develop will help researchers better characterize the structure of their everyday learning environments. It is also important that researchers build corpora that are openly available to the scientific community. Such open datasets enable researchers to collaborate in parsing this complex data and allow for researchers to approach questions across many domains of development. Our contribution to these aims is a corpus of naturalistic, egocentric video data in the preschool classroom for which parents have consented to broad sharing.    

Overall, we built a corpus of child-centered video data in the preschool classroom and computed metrics that characterized the language environment across many activities and speakers. Together, these findings emphasize a dynamic, non-uniform preschool language environment that is structured by the specific activity contexts in which children engage.

# Acknowledgements

We thank the parents, children, teachers, and administrative staff at the nursery school for making this work possible. This work was funded by a gift from the Schmidt Futures Foundation to Michael C. Frank.

# References 
```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
